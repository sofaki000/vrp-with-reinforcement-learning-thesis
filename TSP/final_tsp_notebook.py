# -*- coding: utf-8 -*-
"""thesis-tsp-official.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MZv6WIH_t3Q9b7lwNVtqSVtAX9uEgC5V

<h2>Επίλυση προβλήματος περιοδεύοντος πωλητή με ενισχυτική μάθηση</h2>

<p>Στο παρόν σημειωματάριο θα λύσουμε το πρόβλημα του περιοδεύοντος πωλητή με ενισχυτική μάθηση. </p>
<p>Εκπαιδεύουμε ένα νευρωνικό δίκτυο το οποίο λαμβάνει τις συντεταγμένες των πόλεων που θέλουμε να επισκεφτούμε και επιστρέφει μια κατανομή πιθανοτήτων επίσκεψης της κάθε πόλης κάθε χρονική στιγμή. Το τελικό αποτέλεσμα είναι η εύρεση της σειράς με την οποία θα πρέπει να επισκεφτούμε τις πόλεις με τον πιο αποδοτικό τρόπο. </p>
<p>Η επιβράβευση του πράκτορα ενισχυτικής μάθησης είναι η αρνητική απόσταση που διανύει το φορτηγό για να επισκεφτεί όλες τις πόλεις με τη σειρά που προτείνει το μοντέλο. Το μοντέλο χρησιμοποιεί έναν αλγόριθμο κλίσης (policy gradient) για να βελτιστοποιήσει την πολιτική του πράκτορα (policy) για να μεγιστοποιήσει την επιβράβευση.</p>
"""

# Commented out IPython magic to ensure Python compatibility.
import math
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import time
from IPython.display import clear_output
from tqdm import tqdm

import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt

# %matplotlib inline

"""Δημιουργούμε τα δεδομένα του προβλήματος που είναι οι συντεταγμένες των πόλεων."""


class TSPDataset(Dataset):
    def __init__(self, num_nodes, num_samples, random_seed=111):
        super(TSPDataset, self).__init__()
        torch.manual_seed(random_seed)

        self.data_set = []
        for l in tqdm(range(num_samples)):
            x = torch.FloatTensor(2, num_nodes).uniform_(0, 1)
            self.data_set.append(x)

        self.size = len(self.data_set)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return self.data_set[idx]


class TSPDatasetGaussian(Dataset):
    def __init__(self, num_nodes, num_samples, random_seed=111):
        super(TSPDatasetGaussian, self).__init__()
        torch.manual_seed(random_seed)

        self.data_set = []
        for l in tqdm(range(num_samples)):
            m = torch.distributions.normal.Normal(torch.tensor(0.0), torch.tensor(1.0))
            x = m.sample(sample_shape=(2, num_nodes))
            self.data_set.append(x)

        self.size = len(self.data_set)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return self.data_set[idx]


"""Δημιουργούμε τις κλάσεις με τα δεδομένα του προβλήματος"""

train_size = 100000
val_size = 1000

train_20_dataset = TSPDataset(20, train_size)
val_20_dataset = TSPDataset(20, val_size)
train_20_datasetGauss = TSPDatasetGaussian(20, train_size)
val_20_datasetGauss = TSPDatasetGaussian(20, val_size)

train_50_dataset = TSPDataset(50, train_size)
val_50_dataset = TSPDataset(50, val_size)

train_100_dataset = TSPDataset(100, train_size)
val_100_dataset = TSPDataset(100, val_size)

"""<h3>Ανταμοιβή</h3>
<p>
Δίνοντας ως είσοδο τις συντεταγμένες των πόλεων, έστω ν ο αριθμός των πόλεων, ο στόχος είναι η εύρεση της σειράς π των πόλεων που πρέπει να επισκεφτούμε ώστε το συνολικό μήκος της διαδρομής που διανύσαμε να είναι ελάχιστο. Ορίζουμε την συνάρτηση που υπολογίζει το μήκος των διαδρομών.
</p>
"""


def reward(sample_solution):
    """
    sample_solution seq_len of [batch_size]
    """
    batch_size = sample_solution[0].size(0)
    n = len(sample_solution)
    tour_len = Variable(torch.zeros([batch_size]))

    for i in range(n - 1):
        tour_len += torch.norm(sample_solution[i] - sample_solution[i + 1], dim=1)

    tour_len += torch.norm(sample_solution[n - 1] - sample_solution[0], dim=1)

    return tour_len


"""<h3>Μηχανισμοί προσοχής</h3>
<p>
Χρησιμοποιούμε δυο μηχανισμούς προσοχής που λέγονται γινόμενο ("Dot") και "Bahdanau".</a></p>

$$
a_t(s) = align(h_t, \bar h_s)  = \dfrac{exp(score(h_t, \bar h_s))}{\sum_{s'} exp(score(h_t, \bar h_{s'}))}
$$

$$
score(h_t, \bar h_s) =
\begin{cases}
h_t ^\top \bar h_s & Dot \\
v_a ^\top \tanh(\textbf{W}_a [ h_t ; \bar h_s ]) & Bahdanau
\end{cases}
$$
"""


class Attention(nn.Module):
    def __init__(self, hidden_size, use_tanh=False, C=10, name='Bahdanau'):
        super(Attention, self).__init__()

        self.use_tanh = use_tanh
        self.C = C
        self.name = name

        if name == 'Bahdanau':
            self.W_query = nn.Linear(hidden_size, hidden_size)
            self.W_ref = nn.Conv1d(hidden_size, hidden_size, 1, 1)

            V = torch.FloatTensor(hidden_size)
            self.V = nn.Parameter(V)
            self.V.data.uniform_(-(1. / math.sqrt(hidden_size)), 1. / math.sqrt(hidden_size))

    def forward(self, query, ref):
        """
        Args:
            query: [batch_size x hidden_size]
            ref:   ]batch_size x seq_len x hidden_size]
        """

        batch_size = ref.size(0)
        seq_len = ref.size(1)

        if self.name == 'Bahdanau':
            ref = ref.permute(0, 2, 1)
            query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]
            ref = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]
            expanded_query = query.repeat(1, 1, seq_len)  # [batch_size x hidden_size x seq_len]
            V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size x 1 x hidden_size]
            logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)

        elif self.name == 'Dot':
            query = query.unsqueeze(2)
            logits = torch.bmm(ref, query).squeeze(2)  # [batch_size x seq_len x 1]
            ref = ref.permute(0, 2, 1)

        else:
            raise NotImplementedError

        if self.use_tanh:
            logits = self.C * F.tanh(logits)
        else:
            logits = logits
        return ref, logits


"""<h3>Ενσωμάτωση γράφου</h3>
<p> Χρησιμοποιούμε μια ενσωμάτωση γράφου για τα χαρακτηριστικά που θα λάβει το μοντέλο. Για πιο εξελιγμένες μεθόδους ενσωμάτωσης υπάρχει αυτό το άρθρο <a href="https://arxiv.org/pdf/1705.02801.pdf">Graph Embedding Techniques, Applications, and Performance: A Survey</a>
</p>
"""


class GraphEmbedding(nn.Module):
    def __init__(self, input_size, embedding_size):
        super(GraphEmbedding, self).__init__()
        self.embedding_size = embedding_size

        self.embedding = nn.Parameter(torch.FloatTensor(input_size, embedding_size))
        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))

    def forward(self, inputs):
        batch_size = inputs.size(0)
        seq_len = inputs.size(2)
        embedding = self.embedding.repeat(batch_size, 1, 1)
        embedded = []
        inputs = inputs.unsqueeze(1)
        for i in range(seq_len):
            embedded.append(torch.bmm(inputs[:, :, :, i].float(), embedding))
        embedded = torch.cat(embedded, 1)
        return embedded


class ConvolutionalGraphEmbedding(nn.Module):
    def __init__(self, input_size, embedding_size):
        super(ConvolutionalGraphEmbedding, self).__init__()
        self.embedding_size = embedding_size
        self.embedding = nn.Conv1d(input_size, embedding_size, kernel_size=1)

    def forward(self, inputs):
        batch_size = inputs.size(0)
        embedded = []
        inputs = inputs.unsqueeze(1)
        for i in range(batch_size):
            el = self.embedding(inputs[i, 0])
            embedded.append(torch.unsqueeze(el, 0))
        embedded = torch.cat(embedded, 0)
        return embedded.transpose(2, 1)


"""<h3>Δίκτυα δείκτη (<a href="https://arxiv.org/abs/1506.03134">Pointer Networks
</a>)</h3>
Το μοντέλο αυτό λύνει το πρόβλημα εύρεσης ακολουθίας εξόδου μεταβλητού μεγέθους χρησιμοποιώντας εναν μηχανισμό προσοχής. Ο μηχανισμός αυτός προσοχής, αντί να χρησιμοποιεί την προσοχή για να "αναμείξει" τις κρυμμένες καταστάσεις του κωδικοποιητή σε ένα διάνυσμα πλαισίου (context vector) σε κάθε βήμα αποκωδικοποίησης, χρησιμοποιεί την προσοχή για να "δείξει" σε ένα μέλος της ακολουθίας εισόδου και να το δώσει ως αποτέλεσμα. Αυτό έχει ως συνέπεια να μην χρειάζεται να ξέρουμε εκ των προτέρων το μήκος της ακολουθίας εξόδου. Συνεπώς το ίδιο μοντέλο μπορεί να εκπαιδευτεί για διαφορετικά μήκοι ακολουθίας πόλεων (πχ για 20 ή 50 πόλεις).

"""


class PointerNet(nn.Module):
    def __init__(self,
                 embedding_size,
                 hidden_size,
                 seq_len,
                 n_glimpses,
                 tanh_exploration,
                 use_tanh,
                 attention,
                 embedding="Graph"):
        super(PointerNet, self).__init__()

        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.n_glimpses = n_glimpses
        self.seq_len = seq_len

        if embedding == "Graph":
            self.embedding = GraphEmbedding(2, embedding_size)
        elif embedding == "Conv":
            self.embedding = ConvolutionalGraphEmbedding(2, embedding_size)

        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)
        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)
        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, name=attention)
        self.glimpse = Attention(hidden_size, use_tanh=False, name=attention)

        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))
        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))

    def apply_mask_to_logits(self, logits, mask, idxs):
        batch_size = logits.size(0)
        clone_mask = mask.clone()

        if idxs is not None:
            clone_mask[[i for i in range(batch_size)], idxs.data] = 1
            logits[clone_mask] = -np.inf

        return logits, clone_mask

    def forward(self, inputs):
        """
        Args:
            inputs: [batch_size x 1 x sourceL]
        """
        batch_size = inputs.size(0)
        seq_len = inputs.size(2)
        assert seq_len == self.seq_len

        embedded = self.embedding(inputs)
        encoder_outputs, (hidden, context) = self.encoder(embedded)

        prev_probs = []
        prev_idxs = []
        mask = torch.zeros(batch_size, seq_len).byte()

        idxs = None

        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)

        for i in range(seq_len):

            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))

            query = hidden.squeeze(0)
            for i in range(self.n_glimpses):
                ref, logits = self.glimpse(query, encoder_outputs)
                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)

            _, logits = self.pointer(query, encoder_outputs)
            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
            probs = F.softmax(logits)

            idxs = probs.multinomial(1).squeeze(1)

            for old_idxs in prev_idxs:
                if old_idxs.eq(idxs).data.any():
                    print(seq_len)
                    print(' RESAMPLE!')
                    idxs = probs.multinomial(1).squeeze(1)
                    break

            decoder_input = embedded[[i for i in range(batch_size)], idxs.data, :]

            prev_probs.append(probs)
            prev_idxs.append(idxs)

        return prev_probs, prev_idxs


"""<h3>Βελτιστοποίηση πολιτικής με τον αλγόριθμο κλίσης πολιτικής</h3>
<p>Χρησιμοποιούμε ενισχυτική μάθηση με πολιτική (model-free policy-based Reinforcement Learning) με σκοπό την εκπαίδευση ενός δικτύου δείκτη με τον αλγόριθμο REINFORCE.</p>
"""


class CombinatorialRL(nn.Module):
    def __init__(self,
                 embedding_size,
                 hidden_size,
                 seq_len,
                 n_glimpses,
                 tanh_exploration,
                 use_tanh,
                 reward,
                 attention,
                 embedding):
        super(CombinatorialRL, self).__init__()
        self.reward = reward

        self.actor = PointerNet(
            embedding_size,
            hidden_size,
            seq_len,
            n_glimpses,
            tanh_exploration,
            use_tanh,
            attention,
            embedding)

    def forward(self, inputs):
        """
        Args:
            inputs: [batch_size, input_size, seq_len]
        """
        batch_size = inputs.size(0)
        input_size = inputs.size(1)
        seq_len = inputs.size(2)

        probs, action_idxs = self.actor(inputs)

        actions = []
        inputs = inputs.transpose(1, 2)
        for action_id in action_idxs:
            actions.append(inputs[[x for x in range(batch_size)], action_id.data, :])

        action_probs = []
        for prob, action_id in zip(probs, action_idxs):
            action_probs.append(prob[[x for x in range(batch_size)], action_id.data])

        R = self.reward(actions)

        return R, action_probs, actions, action_idxs


"""Ορίζουμε τις υπερπαραμέτρους του δικτύου."""

embedding_size = 128
hidden_size = 128
n_glimpses = 1
tanh_exploration = 10
use_tanh = True
batch_size = 128
beta = 0.9
max_grad_norm = 2.

"""Δημιουργούμε τα μοντέλα που θα εκπαιδεύσουμε."""

tsp_20_model_dot_attention = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward,
    embedding="Graph",
    attention="Dot")

tsp_20_model_dot_attention_gaussian = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding="Graph",
    attention="Dot")

tsp_50_model_bahdanau_attention = CombinatorialRL(
    embedding_size,
    hidden_size,
    50,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding="Graph",
    attention="Bahdanau")

tsp_20_model_conv_embedding = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward,
    embedding="Conv",
    attention="Dot")

"""<h3>Κλάση για εκπαίδευση μοντέλου</h3>

Στη συνέχεια ορίζουμε μια κλάση στην οποία εκπαιδεύεται το μοντέλο.
"""


class TrainModel:
    def __init__(self, model, train_dataset, val_dataset, batch_size=128, threshold=None, max_grad_norm=2.,
                 modelName="model.pt"):
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.batch_size = batch_size
        self.threshold = threshold
        self.modelName = modelName
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)

        self.actor_optim = optim.Adam(model.actor.parameters(), lr=1e-4)
        self.max_grad_norm = max_grad_norm

        self.train_tour = []
        self.val_tour = []
        self.advantage = []
        self.train_loss = []

        self.epochs = 0

    def train_and_validate(self, n_epochs):
        critic_exp_mvg_avg = torch.zeros(1)
        start_time = time.time()

        for epoch in range(n_epochs):
            loss_at_epoch = 0.0
            distance = 0.0
            val_distance = 0.0
            advantage_at_epoch = 0.0

            for batch_id, sample_batch in enumerate(self.train_loader):
                self.model.train()

                inputs = Variable(sample_batch)

                R, probs, actions, actions_idxs = self.model(inputs)

                if batch_id == 0:
                    critic_exp_mvg_avg = R.mean()
                else:
                    critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())

                advantage = R - critic_exp_mvg_avg
                advantage_at_epoch += advantage.detach().mean().item()

                logprobs = 0
                for prob in probs:
                    logprob = torch.log(prob)
                    logprobs += logprob
                logprobs[logprobs < -1000] = 0.

                reinforce = advantage * logprobs
                actor_loss = reinforce.mean()

                self.actor_optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm(self.model.actor.parameters(),
                                              float(self.max_grad_norm), norm_type=2)

                self.actor_optim.step()

                critic_exp_mvg_avg = critic_exp_mvg_avg.detach()

                loss_at_epoch += actor_loss.detach().item()
                distance += R.mean().data.item()

                if batch_id % 100 == 0:
                    self.model.eval()
                    for val_batch in self.val_loader:
                        inputs = Variable(val_batch)

                        R, probs, actions, actions_idxs = self.model(inputs)
                        val_distance += R.mean().data.item()

            # epoch finished

            self.val_tour.append(val_distance)
            self.train_loss.append(loss_at_epoch)
            self.train_tour.append(distance)
            self.advantage.append(advantage_at_epoch)
            self.plotTrainAndValidationTourLength(self.epochs)
            self.plotTrainLossAndAdvantage(self.epochs)

            torch.save(self.model.state_dict(), self.modelName)
            if self.threshold and self.train_tour[-1] < self.threshold:
                print(f"Early stopping at epoch {epoch}")
                break

            self.epochs += 1

        # Training finished
        training_time_in_secs = time.time() - start_time
        print(f'Trainig for {training_time_in_secs // 60} minutes')

    def plotTrainLossAndAdvantage(self, epoch):
        plt.figure(figsize=(20, 5))
        plt.subplot(131)
        plt.ylabel("Training loss")
        plt.xlabel("Epoch")
        plt.title('Loss: epoch %s loss %s' % (epoch, self.train_tour[-1] if len(self.train_tour) else 'collecting'))
        plt.plot(self.train_loss)
        plt.grid()
        plt.subplot(132)
        plt.title(
            'Advantage: epoch %s reward %s' % (epoch, self.advantage[-1] if len(self.advantage) else 'collecting'))
        plt.plot(self.advantage)
        plt.grid()
        plt.ylabel("Training advantage")
        plt.xlabel("Epoch")
        plt.savefig(f"trainLossAndAdvantage{self.modelName}.png")

        plt.show()
        plt.clf()

    def plotTrainAndValidationTourLength(self, epoch):
        clear_output(True)
        plt.figure(figsize=(20, 5))
        plt.subplot(131)
        plt.ylabel("Tour length")
        plt.xlabel("Epoch")
        plt.title('train tour length: epoch %s,tour length %s' % (
            epoch, self.train_tour[-1] if len(self.train_tour) else 'collecting'))
        plt.plot(self.train_tour)
        plt.grid()
        plt.subplot(132)
        plt.title('val tour length: epoch %s,tour length %s' % (
        epoch, self.val_tour[-1] if len(self.val_tour) else 'collecting'))
        plt.plot(self.val_tour)
        plt.grid()
        plt.savefig(f"tourLength{self.modelName}.png")

        plt.show()
        plt.clf()


"""Πρόβλημα περιοδεύοντος πωλητή για 20 πόλεις με μηχανισμό προσοχής γινομένου."""

tsp_20_train = TrainModel(tsp_20_model_dot_attention,
                          train_20_dataset,
                          val_20_dataset,
                          modelName="tsp_20_dot_attention.pt",
                          threshold=3.99)
tsp_20_train.train_and_validate(15)

"""<h2>TSP 50 Results of Pointer Network with Bahdanau Attention</h2>

"""

train_50_train = TrainModel(tsp_50_model_bahdanau_attention,
                            train_50_dataset,
                            val_50_dataset,
                            modelName="tsp_50_bahdanau_attention.pt",
                            threshold=6.4)
train_50_train.train_and_validate(10)

train_50_train = TrainModel(tsp_20_model_dot_attention_gaussian,
                            train_20_datasetGauss,
                            val_20_datasetGauss,
                            modelName="tsp_20_gaussian_data_dot.pt",
                            threshold=6.4)
train_50_train.train_and_validate(15)

train_20_train_conv_embedding = TrainModel(tsp_20_model_conv_embedding,
                                           train_20_datasetGauss,
                                           val_20_datasetGauss,
                                           modelName="tsp_20_dot_convEmbedding.pt",
                                           threshold=6.4)

train_20_train_conv_embedding.train_and_validate(15)

"""Στη συνέχεια ορίζουμε τη συνάρτηση για την αναπαράσταση της διαδρομής που ακολουθεί ο πωλητής για διάφορες πόλεις"""


def render(data_batch, actions_batch, R_batch, idx_arr, name_suffix=''):
    num_cols = len(idx_arr)

    fig, ax = plt.subplots(nrows=1, ncols=num_cols, figsize=(5 * num_cols, 4))

    # data plotting
    data = [data_batch[i].cpu() for i in idx_arr]
    for i, idx in enumerate(idx_arr):
        x = torch.index_select(data[i], 0, torch.tensor([0]))
        y = torch.index_select(data[i], 0, torch.tensor([1]))
        ax[i].plot(x, y, 'w*')

    # roads plotting
    actions = [torch.index_select(a.cpu(), 0, torch.tensor(idx_arr)) for a in actions_batch]
    R = torch.index_select(R_batch.cpu(), 0, torch.tensor(idx_arr))
    for i, idx in enumerate(idx_arr):
        x = [torch.index_select(a[i], 0, torch.tensor([0])) for a in actions]
        y = [torch.index_select(a[i], 0, torch.tensor([1])) for a in actions]
        ax[i].plot(x[0], y[0], 'r^', ms=10)  # start
        ax[i].plot(x[-1], y[-1], 'bs', ms=10)  # finish
        ax[i].plot(0, 0, 'wo', ms=10)  # finish
        ax[i].plot([0] + x + [0], [0] + y + [0])

        ax[i].set_title(f'idx = {idx} reward={round(R[i].item(), 2)}')

    # styles
    plt.suptitle('Travelling salesman tour ' + name_suffix)
    for i, idx in enumerate(idx_arr):
        ax[i].axes.xaxis.set_visible(False)
        ax[i].axes.yaxis.set_visible(False)
        ax[i].spines['top'].set_visible(False)
        ax[i].spines['right'].set_visible(False)
        ax[i].spines['bottom'].set_visible(False)
        ax[i].spines['left'].set_visible(False)
        ax[i].set_facecolor((0, 0, 0, 1))

    plt.show()


"""Ορίζουμε νέα δεδομένα για να τεστάρουμε τα μοντέλα μας."""

test_size = 2
test_20_dataset = TSPDataset(20, test_size)
train_loader = DataLoader(test_20_dataset, batch_size=100, shuffle=True, num_workers=1)

"""Στη συνέχεια φορτώνουμε τα μοντέλα που εκπαιδεύσαμε και βλέπουμε τα αποτελέσματα για ορισμένες πόλεις."""

tsp_dot_attention_model20_path = "model.pt"  # "tsp_20_model_dot_attention.pt"
tsp_dot_attention_model_gaussian_data20_path = "tsp_20_model_dot_attention_gaussian.pt"
tsp_bahdanau_attention_model_gaussian_data50_path = "tsp_50_model_bahdanau_attention.pt"
tsp_dot_attention_convEmbedding_model20_path = "tsp_20_model_conv_embedding.pt"

tsp_dot_attention_model20_trained = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding="Graph",
    attention="Dot")
tsp_dot_attention_model20_trained.load_state_dict(torch.load(tsp_dot_attention_model20_path))
tsp_dot_attention_model20_trained.eval()

tsp_dot_attention_model_gaussian_data20_trained = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding="Graph",
    attention="Dot")
tsp_dot_attention_model_gaussian_data20_trained.load_state_dict(
    torch.load(tsp_dot_attention_model_gaussian_data20_path))
tsp_dot_attention_model_gaussian_data20_trained.eval()

tsp_bahdanau_attention_model_gaussian_data50_trained = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward,
    embedding="Graph",
    attention="Bahdanau")
tsp_bahdanau_attention_model_gaussian_data50_trained.load_state_dict(
    torch.load(tsp_bahdanau_attention_model_gaussian_data50_path))
tsp_bahdanau_attention_model_gaussian_data50_trained.eval()

tsp_dot_attention_convEmbedding_model20_trained = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding="Conv",
    attention="Dot")
tsp_dot_attention_convEmbedding_model20_trained.load_state_dict(
    torch.load(tsp_dot_attention_convEmbedding_model20_path))
tsp_dot_attention_convEmbedding_model20_trained.eval()

# We compare with an untrained model
tsp_20_model_untrained = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward,
    attention="Dot")
tsp_20_model_untrained.eval()

for batch_id, sample_batch in enumerate(train_loader):
    inputs = Variable(sample_batch)
    R_1, probs_1, actions_1, actions_idxs_trained1 = tsp_dot_attention_model20_trained(inputs)
    R_2, probs_2, actions_2, actions_idxs_trained2 = tsp_dot_attention_model_gaussian_data20_trained(inputs)
    R_3, probs_3, actions_3, actions_idxs_trained3 = tsp_bahdanau_attention_model_gaussian_data50_trained(inputs)
    R_4, probs_4, actions_4, actions_idxs_trained4 = tsp_dot_attention_convEmbedding_model20_trained(inputs)

    R, probs, actions, actions_idxs = tsp_20_model_untrained(inputs)

    # TODO: to reward bgainei megalutero sto untrained model!
    render(inputs, actions_1, R_1, [0, 1], name_suffix='Model with dot attention, 20 cities')
    render(inputs, actions_2, R_2, [0, 1], name_suffix='trained_model, dot attention 20 cities')
    render(inputs, actions_1, R_1, [0, 1], name_suffix='trained_model, dot attention 20 cities')
    render(inputs, actions_1, R_1, [0, 1], name_suffix='trained_model, dot attention 20 cities')
    render(inputs, actions, R, [0, 1], name_suffix='Untrained model')
